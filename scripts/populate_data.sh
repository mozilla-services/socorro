#!/bin/bash

# This Source Code Form is subject to the terms of the Mozilla Public
# License, v. 2.0. If a copy of the MPL was not distributed with this
# file, You can obtain one at http://mozilla.org/MPL/2.0/.

set -e

# Pulls down NUM_CRASHES crashes per day for the last NUM_DAYS days, syncs the
# crash data with local S3 container, and sends all crash ids to # the local
# RabbitMQ queue for processing.
#
# Note: This assumes you've run "make dockersetup" and "make dockerupdatedata"
# already.
#
# Usage:
#
#    $ ./docker/as_me.sh bash
#    you@processor:/app$ ./scripts/populate_data.sh
#
# After running this script, run the processor:
#
#    $ docker-compose up processor
#
# Wait for crashes to process. Then you're good to go!

NUM_DAYS=8
NUM_CRASHES=15
DATADIR=./crashdata_populate_tmp

DELTA=0
STARTDATE=$(date --date="-${NUM_DAYS} days" +%Y-%m-%d)

function cleanup {
    # Cleans up files generated by the script
    rm -rf "${DATADIR}"
}

# Set up cleanup function to run on script exit
trap cleanup EXIT

mkdir ${DATADIR}

echo "Fetching data for ${NUM_CRASHES} crashes per day for last ${NUM_DAYS} days..."
while [ ${DELTA} -lt ${NUM_DAYS} ]
do
    THISDATE=$(date --date="${STARTDATE} ${DELTA} days" +%Y-%m-%d)
    echo "Working on ${THISDATE}..."
    ./scripts/fetch_crashids.py --num=${NUM_CRASHES} --date=${THISDATE} | ./scripts/fetch_crash_data.py ${DATADIR}

    DELTA=$((DELTA + 1))
done

echo "Sync contents to S3 bucket..."
(./scripts/socorro_aws_s3.sh mb s3://dev_bucket/ || true) 2> /dev/null
./scripts/socorro_aws_s3.sh cp --recursive ${DATADIR} s3://dev_bucket/

echo "Add crashids to socorro.normal processing queue..."
./scripts/find_crashids.py ${DATADIR} | ./scripts/add_crashid_to_queue.py socorro.normal

echo ""
echo "****************************************************************"
echo "Data has been pulled down and synced to S3. Run the processor to"
echo "process crashes. After that, you are good to go!"
echo "****************************************************************"
