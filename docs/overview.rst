========
Overview
========

.. contents::
   :local:


What is Socorro?
================

Socorro is a crash ingestion pipeline.

The crash ingestion pipeline that we have at Mozilla looks like this:

.. graphviz::

   digraph G {
      rankdir=LR;
      splines=lines;

      subgraph coll {
         rank=same;

         client [shape=box3d, label="firefox"];
         collector [shape=rect, label="collector"];
      }

      subgraph stores {
         rank=same;

         s3raw [shape=tab, label="S3 (Raw)", style=filled, fillcolor=gray];
         sqs [shape=tab, label="SQS", style=filled, fillcolor=gray];
      }

      processor [shape=rect, label="processor"];

      subgraph stores2 {
         rank=same;

         postgres [shape=tab, label="Postgres", style=filled, fillcolor=gray];
         elasticsearch [shape=tab, label="Elasticsearch", style=filled, fillcolor=gray];
         s3telemetry [shape=tab, label="S3 (Telemetry)", style=filled, fillcolor=gray];
         s3processed [shape=tab, label="S3 (Processed)", style=filled, fillcolor=gray];
      }

      subgraph processing {
         rank=same;

         crontabber [shape=rect, label="crontabber"];
         webapp [shape=rect, label="webapp"];
         telemetry [shape=box3d, label="telemetry"];
      }


      client -> collector [label="HTTP"];
      collector -> s3raw [label="save raw"];
      collector -> sqs [label="publish"];

      sqs -> processor [label="crash id"];
      s3raw -> processor [label="load raw"];
      webapp -> processor [label="betaversionrule"];
      processor -> { s3processed, elasticsearch, s3telemetry } [label="save processed"];

      postgres -> webapp;
      webapp -> postgres;
      s3raw -> webapp [label="load raw"];
      s3processed -> webapp [label="load processed"];
      elasticsearch -> webapp;

      postgres -> crontabber;
      crontabber -> postgres;
      elasticsearch -> crontabber;

      s3telemetry -> telemetry [label="telemetry ingestion"];

      { rank=min; client; }
   }


Arrow direction represents actions and flow of information through the
ingestion pipeline.

Important services in the diagram:

* **Collector:** Collects incoming crash reports via HTTP POST. It generates a
  crash id for the crash report It splits the payload into a raw crash with
  crash annotations and a series of minidump files. It saves this crash data to
  AWS S3 and publishes crash ids to AWS SQS for processing.

* **Processor:** Processes crash reports, extracts data from minidumps,
  generates crash signatures, performs other analysis, and saves everything as
  a processed crash.

* **Webapp (aka Crash Stats):** Web user interface for looking at, searching,
  and analyzing crash report data.

* **Crontabber:** Runs periodic housekeeping tasks using cronrun Django
  command.

The collector we use is called Antenna and the code is in
`<https://github.com/mozilla-services/antenna/>`_.

The processor, webapp, and crontabber services are in the Socorro repository
at `<https://github.com/mozilla-services/socorro/>`_.

Let's take a more detailed tour through the crash ingestion pipeline!


A tour through the crash ingestion pipeline
===========================================

Breakpad-style crash report generated by a crash reporter
---------------------------------------------------------

When Firefox crashes, the breakpad client assembles information about the crash
like the stack, contents in registers and memory, and other things in a
minidump format. It also captures crash annotations.

Depending on what kind of crash just happened, a crash reporter dialog may
prompt the user for additional information and whether the user wants to send
the crash report to Mozilla.

Unless the user says "no" or hasn't opted-in [1]_, the crash reporter will send
the crash report as a multipart/form-data payload via an HTTP POST to the
collector.

This process is complicated because each product and platform has different
breakpad client bits, crash annotations, crash reporter dialogs, and other
things and this code is spread out across a bunch of repositories.

.. [1] Sending crash reports is opt-out by default.


.. seealso::

   **Breakpad overview**
     https://chromium.googlesource.com/breakpad/breakpad/+/master/docs/getting_started_with_breakpad.md

   **Crash reporter documentation**
     https://firefox-source-docs.mozilla.org/toolkit/crashreporter/crashreporter/index.html

   **Crash report specification**
     :ref:`crash-report-spec-chapter`

   **Crash Annotations**
     :ref:`annotations-chapter`


Collected by the Collector
--------------------------

The collector (Antenna) is the beginning of the crash ingestion pipeline.

The collector handles the incoming crash reports and does the following:

1. assigns the crash report a unique crash id
2. adds a submitted time stamp and some other metadata to the crash report
3. determines whether Socorro should process this crash report or not

If Socorro shouldn't process this crash report, then the crash report is
rejected and the collector is done.

If Socorro should process this crash report, then the collector returns the
crash id to the crash reporter in the HTTP response. The crash reporter records
the crash id on the user's machine. The user can see crash reports in
``about:crashes``.

The collector saves the crash report data to AWS S3 as a *raw crash* and
*minidump files* in a directory structure like this:

.. code-block:: text

   v2/
     raw_crash/
       000/
         20160513/
           00007bd0-2d1c-4865-af09-80bc02160513    crash annotations and collection metadata
   v1/
     dump_names/
       00007bd0-2d1c-4865-af09-80bc02160513        list of minidumps for this crash
     dump/
       00007bd0-2d1c-4865-af09-80bc02160513        minidump file


A crash id looks like this::

  de1bb258-cbbf-4589-a673-34f800160918
                               ^^^^^^^
                               ||____|
                               |  yymmdd
                               |
                               throttle result instruction


The collector then publishes the crash report id to AWS SQS for processing.

Note that the throttle result instruction character is no longer used and
always set to ``0``.


.. seealso::

   **Code**
     https://github.com/mozilla-services/antenna/

   **Documentation**
     https://antenna.readthedocs.io/


Processed by Processor
----------------------

The processor pulls crash report ids from the AWS SQS queues. It fetches the
raw crash data and minidump files from AWS S3.

It processes the crash report with a pipeline of rules that use the raw crash
and minidumps to generate a processed crash.

One of the rules runs the minidump-stackwalk on the minidump to extract
information about the process and stack. It symbolicates stack symbols. It
determines some other things about the crash.

Another rule generates a crash signature from the stack of the crashing thread.
We use crash signatures to group crashes that have similar symptoms so that we
can more easily see trends and causes.

There are other rules, too.

After the crash gets through the processing pipeline, the processed crash is
saved to several places:

1. AWS S3
2. Elasticsearch
3. AWS S3 (different bucket) to be ingested into the Telemetry data set

.. seealso::

   **Code**
     https://github.com/mozilla-services/socorro/

   **Documentation**
     https://socorro.readthedocs.io/

   **Stack walking**
     https://chromium.googlesource.com/breakpad/breakpad/+/master/docs/stack_walking.md

   **Breakpad symbols files format**
     https://chromium.googlesource.com/breakpad/breakpad/+/master/docs/symbol_files.md

   **Mozilla symbols server**
     https://symbols.mozilla.org/

   **Socorro processor documentation**
     :ref:`processor-chapter`

   **Signature generation**
     :ref:`signaturegeneration-chapter`


Investigated with Webapp aka Crash Stats
----------------------------------------

The webapp is located at `<https://crash-stats.mozilla.org>`_.

The webapp lets you search through crash reports and facet on aspects of them
with `Super Search
<https://crash-stats.mozilla.org/search/?product=Firefox&_dont_run=1>`_.

The webapp shows `Top Crashers
<https://crash-stats.mozilla.org/topcrashers/?product=Firefox>`_.

The webapp has a `set of APIs <https://crash-stats.mozilla.org/api/>`_ for
accessing data.

You can create an account in the webapp by logging in.

By default, information in a crash report that's personally identifiable
information is hidden. This includes the user's email address and the url the
user was visiting when Firefox crashed.


.. seealso::

   **Code**
     https://github.com/mozilla-services/socorro/

   **Documentation**
     https://socorro.readthedocs.io/

   **Crash Stats user documentation**
     https://crash-stats.mozilla.org/documentation/

   **Crash Stats Super search**
     https://crash-stats.mozilla.org/search/?product=&_dont_run=1

   **Crash Stats APIs**
     https://crash-stats.mozilla.org/api/

   **Privacy policy**
     https://www.mozilla.org/en-US/privacy/websites/

   **Socorro webapp documentation**
     :ref:`webapp-chapter`


Housekeeping with cronrun
-------------------------

We have a ``cronrun`` Django command that acts as a self-healing command runner
that can run any Django command with specified arguments at scheduled times.
We use it to run jobs that perform housekeeping functions in the crash
ingestion pipeline like:

1. updating product/version information for the Beta version lookup
2. updating data about bugs associated with crash signatures
3. updating "first time we saw this signature" type information

cronrun jobs that fail are re-run. Some cronrun jobs are set up to backfill, so
if they fail, they will eventually run for all the times they needed to.

.. seealso::

   **Code (Jobs)**
     https://github.com/mozilla-services/socorro/

   **Socorro scheduled tasks (cronrun) documentation**
     :ref:`cron-chapter`


Sent to Telemetry (External system)
-----------------------------------

Socorro exports a subset of crash data to Telemetry where it can be queried. It's in
the ``socorro_crash`` dataset.

The exported data is considered publicly-safe--there's no protected data in it.

See :ref:`telemetry-chapter` for more details.
